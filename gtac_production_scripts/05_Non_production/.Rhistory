dir
# For every loop, dir resets to main LF directory
dir <- "C:/Users/abhinavshrestha/OneDrive - USDA/Documents/02_TreeMap/temp_dir/02_Landfire/"
paste0(dir, "LF", year_name,"_200_", veg_dataset, ".zip")
paste0(dir, "LF", year_name,"_220_", topo_type, ".zip")
dir <- paste0(dir, "LF_200/Topo/", topo_type, "/")
paste0(dir, "LF", year_name,"_220_", topo_type, ".zip")
# For every loop, dir resets to main LF directory
dir <- "C:/Users/abhinavshrestha/OneDrive - USDA/Documents/02_TreeMap/temp_dir/02_Landfire/"
dir <- paste0(dir, "LF_220/Topo/", topo_type, "/")
paste0(dir, "LF", year_name,"_220_", topo_type, ".zip")
print(url)
url <- paste0(url_baseTopo, year_name, "-LF", year_name, "_", topo_type, url_base_220)
#what url are you working on?
print(url)
for (year_name in years_topo){
print(paste0("Downloading topo data for ", year))
for (topo_type in topoDatasets){
# For every loop, dir resets to main LF directory
dir <- "C:/Users/abhinavshrestha/OneDrive - USDA/Documents/02_TreeMap/temp_dir/02_Landfire/"
##
##### Set appropriate file name
##
if (year_name >= 2020){
url <- paste0(url_baseTopo, year_name, "-LF", year_name, "_", topo_type, url_base_220)
dir <- paste0(dir, "LF_220/Topo/", topo_type, "/")
# create file name
zipfilename <- paste0(dir, "LF", year_name,"_220_", topo_type, ".zip")
}
##
##### Download and extract
##
# create file name for out folder
# filename <- gsub(".zip", "", zipfilename)
# create directory if necessary
# if(!file.exists(dir)) {
#   dir.create(dir, recursive = TRUE)
# }
#what url are you working on?
print(url)
#download files
options(timeout=7200) # set high number for connection timeout (default = 60 s)
print("- downloading...")
# repeat-tryCatch loop to catch connection errors (https://stackoverflow.com/questions/63340463/download-files-until-it-works; https://stackoverflow.com/questions/50624864/skipping-error-files-when-downloading-using-download-file-in-r)
error_count = 0
downloadcount = 0
# repeat{
#   tryCatch({download.file(url, zipfilename, mode = "wb", quite = FALSE)
#     downloadcount <<- downloadcount + 1}, # successful download
#     error = function(e){error_count <<- error_count + 1 # unsuccessful download, add to error count
#     print("Downloading did not work.")
#     return(e)})
#   if (downloadcount > 0){
#     break # stop repeat loop due to successful download
#   }
#
#   if (error_count == maxDownload_count){
#     print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
#     print(paste0("Tried downloading ", maxDownload_count, " times. Please check URL, internet connection, and system."))
#     print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
#     break # stop repeat loop due to maximum download tries reached
#   }
#   print("Retrying...") # error count still less than max download count, repeat loop to retry
#   Sys.sleep(0.5)
# }
print(paste0("successful download (Y = 1, N = 0): ", downloadcount))
print(paste("number of download errors: ", error_count))
print("-- download complete")
print(paste0("- extracting ", year_name, " ", veg_dataset, "..."))
print(paste0("-- location: ", dir))
#unzip
# unzip(zipfilename, exdir = dir, overwrite = TRUE)
print("-- extraction complete")
#remove zipped files
print(paste0("- removing compressed (.zip) folder: ", gsub(dir, "", zipfilename)))
# file.remove(zipfilename)
print(paste0("-- removed ", gsub(dir, "", zipfilename)))
message("moving to next dataset...")
print("---------------------------------------------------------------------")
}
}
for (year_name in years_topo){
print(paste0("Downloading topo data for ", year_name))
for (topo_type in topoDatasets){
# For every loop, dir resets to main LF directory
dir <- "C:/Users/abhinavshrestha/OneDrive - USDA/Documents/02_TreeMap/temp_dir/02_Landfire/"
##
##### Set appropriate file name
##
if (year_name >= 2020){
url <- paste0(url_baseTopo, year_name, "-LF", year_name, "_", topo_type, url_base_220)
dir <- paste0(dir, "LF_220/Topo/", topo_type, "/")
# create file name
zipfilename <- paste0(dir, "LF", year_name,"_220_", topo_type, ".zip")
}
##
##### Download and extract
##
# create file name for out folder
# filename <- gsub(".zip", "", zipfilename)
# create directory if necessary
# if(!file.exists(dir)) {
#   dir.create(dir, recursive = TRUE)
# }
#what url are you working on?
print(url)
#download files
options(timeout=7200) # set high number for connection timeout (default = 60 s)
print("- downloading...")
# repeat-tryCatch loop to catch connection errors (https://stackoverflow.com/questions/63340463/download-files-until-it-works; https://stackoverflow.com/questions/50624864/skipping-error-files-when-downloading-using-download-file-in-r)
error_count = 0
downloadcount = 0
# repeat{
#   tryCatch({download.file(url, zipfilename, mode = "wb", quite = FALSE)
#     downloadcount <<- downloadcount + 1}, # successful download
#     error = function(e){error_count <<- error_count + 1 # unsuccessful download, add to error count
#     print("Downloading did not work.")
#     return(e)})
#   if (downloadcount > 0){
#     break # stop repeat loop due to successful download
#   }
#
#   if (error_count == maxDownload_count){
#     print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
#     print(paste0("Tried downloading ", maxDownload_count, " times. Please check URL, internet connection, and system."))
#     print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
#     break # stop repeat loop due to maximum download tries reached
#   }
#   print("Retrying...") # error count still less than max download count, repeat loop to retry
#   Sys.sleep(0.5)
# }
print(paste0("successful download (Y = 1, N = 0): ", downloadcount))
print(paste("number of download errors: ", error_count))
print("-- download complete")
print(paste0("- extracting ", year_name, " ", veg_dataset, "..."))
print(paste0("-- location: ", dir))
#unzip
# unzip(zipfilename, exdir = dir, overwrite = TRUE)
print("-- extraction complete")
#remove zipped files
print(paste0("- removing compressed (.zip) folder: ", gsub(dir, "", zipfilename)))
# file.remove(zipfilename)
print(paste0("-- removed ", gsub(dir, "", zipfilename)))
message("moving to next dataset...")
print("---------------------------------------------------------------------")
}
}
url_baseTopo <- "https://www.landfire.gov/bulk/downloadfile.php?FNAME=US_Topo_"
url_base_220 <- "_220_CONUS.zip&TYPE=landfire"
years_topo <- c("2020")
topoDatasets <- c("Asp", "Elev", "SlpD")
for (year_name in years_topo){
print(paste0("Downloading topo data for ", year_name))
for (topo_type in topoDatasets){
# For every loop, dir resets to main LF directory
dir <- "C:/Users/abhinavshrestha/OneDrive - USDA/Documents/02_TreeMap/temp_dir/02_Landfire/"
##
##### Set appropriate file name
##
if (year_name >= 2020){
url <- paste0(url_baseTopo, year_name, "-LF", year_name, "_", topo_type, url_base_220)
dir <- paste0(dir, "LF_220/Topo/", topo_type, "/")
# create file name
zipfilename <- paste0(dir, "LF", year_name,"_220_", topo_type, ".zip")
}
##
##### Download and extract
##
# create file name for out folder
# filename <- gsub(".zip", "", zipfilename)
# create directory if necessary
# if(!file.exists(dir)) {
#   dir.create(dir, recursive = TRUE)
# }
#what url are you working on?
print(url)
#download files
options(timeout=7200) # set high number for connection timeout (default = 60 s)
print("- downloading...")
# repeat-tryCatch loop to catch connection errors (https://stackoverflow.com/questions/63340463/download-files-until-it-works; https://stackoverflow.com/questions/50624864/skipping-error-files-when-downloading-using-download-file-in-r)
error_count = 0
downloadcount = 0
# repeat{
#   tryCatch({download.file(url, zipfilename, mode = "wb", quite = FALSE)
#     downloadcount <<- downloadcount + 1}, # successful download
#     error = function(e){error_count <<- error_count + 1 # unsuccessful download, add to error count
#     print("Downloading did not work.")
#     return(e)})
#   if (downloadcount > 0){
#     break # stop repeat loop due to successful download
#   }
#
#   if (error_count == maxDownload_count){
#     print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
#     print(paste0("Tried downloading ", maxDownload_count, " times. Please check URL, internet connection, and system."))
#     print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
#     break # stop repeat loop due to maximum download tries reached
#   }
#   print("Retrying...") # error count still less than max download count, repeat loop to retry
#   Sys.sleep(0.5)
# }
print(paste0("successful download (Y = 1, N = 0): ", downloadcount))
print(paste("number of download errors: ", error_count))
print("-- download complete")
print(paste0("- extracting ", year_name, " ", topo_type, "..."))
print(paste0("-- location: ", dir))
#unzip
# unzip(zipfilename, exdir = dir, overwrite = TRUE)
print("-- extraction complete")
#remove zipped files
print(paste0("- removing compressed (.zip) folder: ", gsub(dir, "", zipfilename)))
# file.remove(zipfilename)
print(paste0("-- removed ", gsub(dir, "", zipfilename)))
message("moving to next dataset...")
print("---------------------------------------------------------------------")
}
}
ptm_TopoStart <- Sys.time()
url_baseTopo <- "https://www.landfire.gov/bulk/downloadfile.php?FNAME=US_Topo_"
url_base_220 <- "_220_CONUS.zip&TYPE=landfire"
years_topo <- c("2020")
topoDatasets <- c("Asp", "Elev", "SlpD")
for (year_name in years_topo){
print(paste0("Downloading topo data for ", year_name))
for (topo_type in topoDatasets){
print(paste0("--- Topo type: ", topo_type))
# For every loop, dir resets to main LF directory
dir <- "C:/Users/abhinavshrestha/OneDrive - USDA/Documents/02_TreeMap/temp_dir/02_Landfire/"
##
##### Set appropriate file name
##
if (year_name >= 2020){
url <- paste0(url_baseTopo, year_name, "-LF", year_name, "_", topo_type, url_base_220)
dir <- paste0(dir, "LF_220/Topo/", topo_type, "/")
# create file name
zipfilename <- paste0(dir, "LF", year_name,"_220_", topo_type, ".zip")
}
##
##### Download and extract
##
# create file name for out folder
# filename <- gsub(".zip", "", zipfilename)
# create directory if necessary
# if(!file.exists(dir)) {
#   dir.create(dir, recursive = TRUE)
# }
#what url are you working on?
print(url)
#download files
options(timeout=7200) # set high number for connection timeout (default = 60 s)
print("- downloading...")
# repeat-tryCatch loop to catch connection errors (https://stackoverflow.com/questions/63340463/download-files-until-it-works; https://stackoverflow.com/questions/50624864/skipping-error-files-when-downloading-using-download-file-in-r)
error_count = 0
downloadcount = 0
# repeat{
#   tryCatch({download.file(url, zipfilename, mode = "wb", quite = FALSE)
#     downloadcount <<- downloadcount + 1}, # successful download
#     error = function(e){error_count <<- error_count + 1 # unsuccessful download, add to error count
#     print("Downloading did not work.")
#     return(e)})
#   if (downloadcount > 0){
#     break # stop repeat loop due to successful download
#   }
#
#   if (error_count == maxDownload_count){
#     print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
#     print(paste0("Tried downloading ", maxDownload_count, " times. Please check URL, internet connection, and system."))
#     print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
#     break # stop repeat loop due to maximum download tries reached
#   }
#   print("Retrying...") # error count still less than max download count, repeat loop to retry
#   Sys.sleep(0.5)
# }
print(paste0("successful download (Y = 1, N = 0): ", downloadcount))
print(paste("number of download errors: ", error_count))
print("-- download complete")
print(paste0("- extracting ", year_name, " ", topo_type, "..."))
print(paste0("-- location: ", dir))
#unzip
# unzip(zipfilename, exdir = dir, overwrite = TRUE)
print("-- extraction complete")
#remove zipped files
print(paste0("- removing compressed (.zip) folder: ", gsub(dir, "", zipfilename)))
# file.remove(zipfilename)
print(paste0("-- removed ", gsub(dir, "", zipfilename)))
message("moving to next dataset...")
print("---------------------------------------------------------------------")
}
}
Sys.time() - ptm_TopoStart
ptm_TopoStart <- Sys.time()
url_baseTopo <- "https://www.landfire.gov/bulk/downloadfile.php?FNAME=US_Topo_"
url_base_220 <- "_220_CONUS.zip&TYPE=landfire"
years_topo <- c("2020")
topoDatasets <- c("Asp", "Elev", "SlpD")
for (year_name in years_topo){
print(paste0("Downloading topo data for ", year_name))
for (topo_type in topoDatasets){
print(paste0("--> Topo type: ", topo_type))
# For every loop, dir resets to main LF directory
dir <- "C:/Users/abhinavshrestha/OneDrive - USDA/Documents/02_TreeMap/temp_dir/02_Landfire/"
##
##### Set appropriate file name
##
if (year_name >= 2020){
url <- paste0(url_baseTopo, year_name, "-LF", year_name, "_", topo_type, url_base_220)
dir <- paste0(dir, "LF_220/Topo/", topo_type, "/")
# create file name
zipfilename <- paste0(dir, "LF", year_name,"_220_", topo_type, ".zip")
}
##
##### Download and extract
##
# create file name for out folder
# filename <- gsub(".zip", "", zipfilename)
# create directory if necessary
# if(!file.exists(dir)) {
#   dir.create(dir, recursive = TRUE)
# }
#what url are you working on?
print(url)
#download files
options(timeout=7200) # set high number for connection timeout (default = 60 s)
print("- downloading...")
# repeat-tryCatch loop to catch connection errors (https://stackoverflow.com/questions/63340463/download-files-until-it-works; https://stackoverflow.com/questions/50624864/skipping-error-files-when-downloading-using-download-file-in-r)
error_count = 0
downloadcount = 0
# repeat{
#   tryCatch({download.file(url, zipfilename, mode = "wb", quite = FALSE)
#     downloadcount <<- downloadcount + 1}, # successful download
#     error = function(e){error_count <<- error_count + 1 # unsuccessful download, add to error count
#     print("Downloading did not work.")
#     return(e)})
#   if (downloadcount > 0){
#     break # stop repeat loop due to successful download
#   }
#
#   if (error_count == maxDownload_count){
#     print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
#     print(paste0("Tried downloading ", maxDownload_count, " times. Please check URL, internet connection, and system."))
#     print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
#     break # stop repeat loop due to maximum download tries reached
#   }
#   print("Retrying...") # error count still less than max download count, repeat loop to retry
#   Sys.sleep(0.5)
# }
print(paste0("successful download (Y = 1, N = 0): ", downloadcount))
print(paste("number of download errors: ", error_count))
print("-- download complete")
print(paste0("- extracting ", year_name, " ", topo_type, "..."))
print(paste0("-- location: ", dir))
#unzip
# unzip(zipfilename, exdir = dir, overwrite = TRUE)
print("-- extraction complete")
#remove zipped files
print(paste0("- removing compressed (.zip) folder: ", gsub(dir, "", zipfilename)))
# file.remove(zipfilename)
print(paste0("-- removed ", gsub(dir, "", zipfilename)))
message("moving to next dataset...")
print("---------------------------------------------------------------------")
}
}
Sys.time() - ptm_TopoStart
ptm_TopoStart <- Sys.time()
url_baseTopo <- "https://www.landfire.gov/bulk/downloadfile.php?FNAME=US_Topo_"
url_base_220 <- "_220_CONUS.zip&TYPE=landfire"
years_topo <- c("2020")
topoDatasets <- c("Asp", "Elev", "SlpD")
for (year_name in years_topo){
print(paste0("Downloading topo data for ", year_name))
for (topo_type in topoDatasets){
ptm_topoTypeStart <- Sys.time()
print(paste0("--> Topo type: ", topo_type))
# For every loop, dir resets to main LF directory
dir <- "C:/Users/abhinavshrestha/OneDrive - USDA/Documents/02_TreeMap/temp_dir/02_Landfire/"
##
##### Set appropriate file name
##
if (year_name >= 2020){
url <- paste0(url_baseTopo, year_name, "-LF", year_name, "_", topo_type, url_base_220)
dir <- paste0(dir, "LF_220/Topo/", topo_type, "/")
# create file name
zipfilename <- paste0(dir, "LF", year_name,"_220_", topo_type, ".zip")
}
##
##### Download and extract
##
# create file name for out folder
# filename <- gsub(".zip", "", zipfilename)
# create directory if necessary
if(!file.exists(dir)) {
dir.create(dir, recursive = TRUE)
}
#what url are you working on?
print(url)
#download files
options(timeout=7200) # set high number for connection timeout (default = 60 s)
print("- downloading...")
# repeat-tryCatch loop to catch connection errors (https://stackoverflow.com/questions/63340463/download-files-until-it-works; https://stackoverflow.com/questions/50624864/skipping-error-files-when-downloading-using-download-file-in-r)
error_count = 0
downloadcount = 0
repeat{
tryCatch({download.file(url, zipfilename, mode = "wb", quite = FALSE)
downloadcount <<- downloadcount + 1}, # successful download
error = function(e){error_count <<- error_count + 1 # unsuccessful download, add to error count
print("Downloading did not work.")
return(e)})
if (downloadcount > 0){
break # stop repeat loop due to successful download
}
if (error_count == maxDownload_count){
print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
print(paste0("Tried downloading ", maxDownload_count, " times. Please check URL, internet connection, and system."))
print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
break # stop repeat loop due to maximum download tries reached
}
print("Retrying...") # error count still less than max download count, repeat loop to retry
Sys.sleep(0.5)
}
print(paste0("successful download (Y = 1, N = 0): ", downloadcount))
print(paste("number of download errors: ", error_count))
print("-- download complete")
print(paste0("- extracting ", year_name, " ", topo_type, "..."))
print(paste0("-- location: ", dir))
#unzip
unzip(zipfilename, exdir = dir, overwrite = TRUE)
print("-- extraction complete")
#remove zipped files
print(paste0("- removing compressed (.zip) folder: ", gsub(dir, "", zipfilename)))
file.remove(zipfilename)
print(paste0("-- removed ", gsub(dir, "", zipfilename)))
message("moving to next dataset...")
print("---------------------------------------------------------------------")
Sys.time() - ptm_topoTypeStart
}
}
# set maximum tries to download
maxDownload_count <- 5
ptm_TopoStart <- Sys.time()
url_baseTopo <- "https://www.landfire.gov/bulk/downloadfile.php?FNAME=US_Topo_"
url_base_220 <- "_220_CONUS.zip&TYPE=landfire"
years_topo <- c("2020")
# topoDatasets <- c("Asp", "Elev", "SlpD")
topoDatasets <- c("Elev", "SlpD")
for (year_name in years_topo){
print(paste0("Downloading topo data for ", year_name))
for (topo_type in topoDatasets){
ptm_topoTypeStart <- Sys.time()
print(paste0("--> Topo type: ", topo_type))
# For every loop, dir resets to main LF directory
dir <- "C:/Users/abhinavshrestha/OneDrive - USDA/Documents/02_TreeMap/temp_dir/02_Landfire/"
##
##### Set appropriate file name
##
if (year_name >= 2020){
url <- paste0(url_baseTopo, year_name, "-LF", year_name, "_", topo_type, url_base_220)
dir <- paste0(dir, "LF_220/Topo/", topo_type, "/")
# create file name
zipfilename <- paste0(dir, "LF", year_name,"_220_", topo_type, ".zip")
}
##
##### Download and extract
##
# create file name for out folder
# filename <- gsub(".zip", "", zipfilename)
# create directory if necessary
if(!file.exists(dir)) {
dir.create(dir, recursive = TRUE)
}
#what url are you working on?
print(url)
#download files
options(timeout=7200) # set high number for connection timeout (default = 60 s)
print("- downloading...")
# repeat-tryCatch loop to catch connection errors (https://stackoverflow.com/questions/63340463/download-files-until-it-works; https://stackoverflow.com/questions/50624864/skipping-error-files-when-downloading-using-download-file-in-r)
error_count = 0
downloadcount = 0
repeat{
tryCatch({download.file(url, zipfilename, mode = "wb", quite = FALSE)
downloadcount <<- downloadcount + 1}, # successful download
error = function(e){error_count <<- error_count + 1 # unsuccessful download, add to error count
print("Downloading did not work.")
return(e)})
if (downloadcount > 0){
break # stop repeat loop due to successful download
}
if (error_count == maxDownload_count){
print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
print(paste0("Tried downloading ", maxDownload_count, " times. Please check URL, internet connection, and system."))
print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
break # stop repeat loop due to maximum download tries reached
}
print("Retrying...") # error count still less than max download count, repeat loop to retry
Sys.sleep(0.5)
}
print(paste0("successful download (Y = 1, N = 0): ", downloadcount))
print(paste("number of download errors: ", error_count))
print("-- download complete")
print(paste0("- extracting ", year_name, " ", topo_type, "..."))
print(paste0("-- location: ", dir))
#unzip
unzip(zipfilename, exdir = dir, overwrite = TRUE)
print("-- extraction complete")
#remove zipped files
print(paste0("- removing compressed (.zip) folder: ", gsub(dir, "", zipfilename)))
file.remove(zipfilename)
print(paste0("-- removed ", gsub(dir, "", zipfilename)))
message("moving to next dataset...")
print("---------------------------------------------------------------------")
Sys.time() - ptm_topoTypeStart
}
}
